# é’ˆå¯¹PL Starç¼ºé™·æ£€æµ‹çš„ç¬¬ä¸€æ­¥æ”¹è¿›
# åªæ”¹è¿›æ ‡å‡†åŒ–éƒ¨åˆ†ï¼Œä¿æŒå…¶ä»–ä»£ç ä¸å˜

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset
from scipy.io import loadmat

class PLStarSegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir, low_prec, high_prec, transform=None,
                 use_global_stats=True,          # ğŸ”¥ å…¨å±€æ ‡å‡†åŒ–å¼€å…³
                 preserve_precision=True):       # ğŸ”¥ ä¿æŒæ•°å€¼ç²¾åº¦å¼€å…³
        
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.low_prec = low_prec
        self.high_prec = high_prec
        self.use_global_stats = use_global_stats
        self.preserve_precision = preserve_precision  # ğŸ”¥ æ–°å¢
        self.image_files = []
        
        # åŸå§‹æ–‡ä»¶æ”¶é›†é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
        all_files = sorted(os.listdir(image_dir))
        for f in all_files:
            if f.endswith(('.png', '.jpg', '.jpeg')):
                self.image_files.append(f)
            if f.endswith('_PLStar.mat'):
                self.image_files.append(f)
        
        print(f"æ‰¾åˆ° {len(self.image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šé¢„è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯
        if self.use_global_stats:
            print("æ­£åœ¨è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯ç”¨äºPL Staræ£€æµ‹...")
            self.global_stats = self._compute_global_stats()
            print(f"å…¨å±€ç»Ÿè®¡å®Œæˆ:")
            print(f"  æ•°å€¼èŒƒå›´: {self.global_stats['min']:.4f} ~ {self.global_stats['max']:.4f}")
            print(f"  æ ‡å‡†åŒ–é˜ˆå€¼: {self.global_stats['low_thresh']:.4f} ~ {self.global_stats['high_thresh']:.4f}")
            print(f"  æœ‰æ•ˆåƒç´ æ¯”ä¾‹: {self.global_stats['valid_ratio']:.1%}")
    
    def _compute_global_stats(self):
        """
        ğŸ”¥ é’ˆå¯¹PL Starçš„å…¨å±€ç»Ÿè®¡è®¡ç®—
        é‡ç‚¹ï¼šä¿æŒå¾®å¼±ä¿¡å·çš„ç²¾åº¦
        """
        all_valid_values = []
        total_pixels = 0
        valid_pixels = 0
        
        # é‡‡æ ·è®¡ç®—ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        sample_count = min(20, len(self.image_files))
        print(f"ä» {sample_count} ä¸ªæ ·æœ¬è®¡ç®—å…¨å±€ç»Ÿè®¡...")
        
        for i, img_file in enumerate(self.image_files[:sample_count]):
            try:
                print(f"  å¤„ç† {i+1}/{sample_count}: {img_file}")
                
                # åŠ è½½å›¾åƒ
                image = self._load_single_image(img_file)
                if image is not None:
                    # ç»Ÿè®¡æœ‰æ•ˆåƒç´ 
                    valid_mask = ~np.isnan(image)
                    valid_data = image[valid_mask]
                    
                    total_pixels += image.size
                    valid_pixels += len(valid_data)
                    
                    # æ”¶é›†æœ‰æ•ˆæ•°å€¼ï¼ˆä¿æŒç²¾åº¦ï¼‰
                    all_valid_values.extend(valid_data.flatten())
                    
                    # æ˜¾ç¤ºè¿™ä¸ªæ ·æœ¬çš„åŸºæœ¬ä¿¡æ¯
                    if len(valid_data) > 0:
                        print(f"    èŒƒå›´: {valid_data.min():.4f} ~ {valid_data.max():.4f}")
                        print(f"    å‡å€¼: {valid_data.mean():.4f}")
                        print(f"    æœ‰æ•ˆåƒç´ : {len(valid_data)}/{image.size} ({len(valid_data)/image.size:.1%})")
                        
            except Exception as e:
                print(f"    è·³è¿‡ {img_file}: {e}")
                continue
        
        if not all_valid_values:
            raise RuntimeError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆåƒç´ è¿›è¡Œç»Ÿè®¡")
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡
        all_valid_values = np.array(all_valid_values, dtype=np.float64)  # ä¿æŒç²¾åº¦
        
        stats = {
            'min': np.min(all_valid_values),
            'max': np.max(all_valid_values), 
            'mean': np.mean(all_valid_values),
            'std': np.std(all_valid_values),
            'low_thresh': np.percentile(all_valid_values, self.low_prec),
            'high_thresh': np.percentile(all_valid_values, self.high_prec),
            'valid_ratio': valid_pixels / total_pixels,
            'total_samples': len(all_valid_values)
        }
        
        return stats
    
    def _load_single_image(self, img_file):
        """åŠ è½½å•ä¸ªå›¾åƒï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰"""
        try:
            file_ext = os.path.splitext(img_file)[1].lower()
            image_path = os.path.join(self.image_dir, img_file)
            
            if file_ext == '.mat':
                image_data = loadmat(image_path)
                return image_data['modifiedMap']
            else:
                # å¯¹äºå›¾åƒæ–‡ä»¶ï¼Œç¡®ä¿åŠ è½½ä¸ºfloatä»¥ä¿æŒç²¾åº¦
                img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)
                if img is not None and img.dtype == np.uint8:
                    # å¦‚æœæ˜¯8ä½å›¾åƒï¼Œéœ€è¦è½¬æ¢å›åŸå§‹èŒƒå›´
                    print(f"    è­¦å‘Š: {img_file} æ˜¯8ä½å›¾åƒï¼Œå¯èƒ½ä¸¢å¤±ç²¾åº¦")
                return img
        except Exception as e:
            print(f"    åŠ è½½å¤±è´¥: {e}")
            return None
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        # ============================================
        # å‰é¢çš„åŠ è½½é€»è¾‘å®Œå…¨ä¿æŒåŸæ ·
        # ============================================
        file_name = self.image_files[idx]
        file_ext = os.path.splitext(file_name)[1].lower()
        image_path = os.path.join(self.image_dir, file_name)
        
        if file_ext == '.mat':
            image_data = loadmat(image_path)
            image = image_data['modifiedMap']
            
            if file_name.endswith('_PLStar.mat'):
                base_name = file_name[:-11]
                mask_file = base_name + "_Mask.mat"
                mask_path = os.path.join(self.mask_dir, mask_file)
            else:
                mask_path = os.path.join(self.mask_dir, file_name)
            
            mask_data = loadmat(mask_path)
            mask = mask_data['maskMap']
        else:
            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            base_name = os.path.splitext(file_name)[0]
            mask_candidates = [
                os.path.join(self.mask_dir, base_name + ext)
                for ext in ('.png', '.jpeg', '.jpg')
            ]
            
            mask_path = None
            for candidate in mask_candidates:
                if os.path.exists(candidate):
                    mask_path = candidate
                    break
            
            if mask_path is None:
                base_name = file_name[:-11]
                mask_file = base_name + "_Mask.mat"
                mask_path = os.path.join(self.mask_dir, mask_file)
            
            mask_ext = os.path.splitext(mask_path)[1].lower()
            if mask_ext == '.mat':
                mask_data = loadmat(mask_path)
                mask = mask_data['maskMap']
            else:
                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        
        if image is None:
            raise RuntimeError(f"Unable to load image: {image_path}")
        if mask is None:
            raise RuntimeError(f"Unable to load mask: {mask_path}")
        
        if len(mask.shape) == 3:
            mask = mask[:, :, 0]
        
        # ============================================
        # è½¬æ¢ä¸ºtensorï¼ˆä¿æŒåŸæ ·ï¼‰
        # ============================================
        if len(image.shape) == 2:
            image = torch.from_numpy(image).float().unsqueeze(0)
        else:
            image = torch.from_numpy(image.astype(np.double).transpose(2, 0, 1)).float()
        
        # ğŸ”¥ é’ˆå¯¹äºŒåˆ†ç±»çš„maskå¤„ç†æ”¹è¿›
        mask = torch.from_numpy(mask.astype(np.uint8)).long()  # æ”¹ä¸ºlongç±»å‹ç”¨äºCE loss
        
        # ============================================
        # ğŸ”¥ğŸ”¥ å…³é”®æ”¹è¿›ï¼šPL Starä¸“ç”¨æ ‡å‡†åŒ– ğŸ”¥ğŸ”¥
        # ============================================
        processed_image = image.clone()
        background_mask = torch.isnan(processed_image)
        
        if self.use_global_stats and hasattr(self, 'global_stats'):
            # âœ… ä½¿ç”¨å…¨å±€ç»Ÿè®¡ï¼ˆæ¨èï¼‰
            stats = self.global_stats
            low_thres = stats['low_thresh']
            high_thres = stats['high_thresh']
            
            # è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            if idx < 3:  # åªå¯¹å‰3ä¸ªæ ·æœ¬æ‰“å°
                print(f"æ ·æœ¬{idx}: ä½¿ç”¨å…¨å±€é˜ˆå€¼ {low_thres:.4f} ~ {high_thres:.4f}")
        else:
            # âŒ åŸæ¥çš„æ–¹æ³•ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸åŒï¼Œä¸æ¨èï¼‰
            valid_values = processed_image[~background_mask]
            if len(valid_values) > 0:
                low_thres = torch.quantile(valid_values, self.low_prec/100.0).item()
                high_thres = torch.quantile(valid_values, self.high_prec/100.0).item()
                if idx < 3:
                    print(f"æ ·æœ¬{idx}: ä¸ªåˆ«é˜ˆå€¼ {low_thres:.4f} ~ {high_thres:.4f}")
            else:
                low_thres, high_thres = 0, 0.1
        
        # ğŸ”¥ æ”¹è¿›çš„æ ‡å‡†åŒ–é€»è¾‘
        if high_thres > low_thres:
            # è£å‰ªåˆ°åˆ†ä½æ•°èŒƒå›´
            processed_image = torch.clamp(processed_image, min=low_thres, max=high_thres)
            
            if self.preserve_precision:
                # âœ… ç›´æ¥æ ‡å‡†åŒ–åˆ°[0,1]ï¼Œä¿æŒæœ€å¤§ç²¾åº¦
                normalized_image = (processed_image - low_thres) / (high_thres - low_thres)
                # NaNåŒºåŸŸè®¾ä¸º0
                normalized_image[background_mask] = 0.0
                
                if idx < 3:
                    valid_normalized = normalized_image[~background_mask]
                    if len(valid_normalized) > 0:
                        print(f"    æ ‡å‡†åŒ–åèŒƒå›´: {valid_normalized.min():.4f} ~ {valid_normalized.max():.4f}")
            else:
                # âŒ åŸæ¥çš„æ–¹æ³•ï¼ˆæŸå¤±ç²¾åº¦ï¼Œä¸æ¨èï¼‰
                normalized_image = ((processed_image - low_thres)/(high_thres - low_thres)) * (255-20) + 20
                normalized_image[background_mask] = 0
                normalized_image = normalized_image / 255.0
        else:
            # å¼‚å¸¸æƒ…å†µå¤„ç†
            normalized_image = torch.zeros_like(processed_image)
            print(f"è­¦å‘Š: æ ·æœ¬{idx}æ ‡å‡†åŒ–é˜ˆå€¼å¼‚å¸¸")
        
        # ============================================
        # æ•°æ®å¢å¼ºï¼ˆä¿æŒåŸæ ·ï¼Œä½†éœ€è¦æ³¨æ„PL Starçš„å¯¹ç§°æ€§ï¼‰
        # ============================================
        if self.transform:
            # æ³¨æ„ï¼šå¯¹äºPL Starï¼Œæ—‹è½¬åº”è¯¥æ˜¯60Â°çš„å€æ•°
            transformed = self.transform(image=normalized_image, mask=mask)
            normalized_image = transformed['image']
            mask = transformed['mask']
        
        return normalized_image, mask


# ğŸ”¥ ä½¿ç”¨ç¤ºä¾‹
def create_pl_star_dataset():
    """
    åˆ›å»ºPL Starä¸“ç”¨æ•°æ®é›†
    """
    dataset = PLStarSegmentationDataset(
        image_dir="path/to/your/wafer/images",
        mask_dir="path/to/your/masks", 
        low_prec=10,                    # æ‚¨çš„åŸå§‹è®¾ç½®
        high_prec=90,                   # æ‚¨çš„åŸå§‹è®¾ç½®
        use_global_stats=True,          # ğŸ”¥ å…³é”®ï¼šå¯ç”¨å…¨å±€æ ‡å‡†åŒ–
        preserve_precision=True,        # ğŸ”¥ å…³é”®ï¼šä¿æŒæ•°å€¼ç²¾åº¦
        transform=None                  # å…ˆä¸åŠ æ•°æ®å¢å¼º
    )
    
    print(f"\nğŸ¯ PL Staræ•°æ®é›†åˆ›å»ºå®Œæˆ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")
    print(f"   ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–: {dataset.use_global_stats}")
    print(f"   ä¿æŒæ•°å€¼ç²¾åº¦: {dataset.preserve_precision}")
    
    return dataset

# ä½¿ç”¨æ–¹æ³•ï¼š
# dataset = create_pl_star_dataset()
